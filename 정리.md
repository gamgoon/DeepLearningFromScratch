# Perceptron 퍼셉트론 알고리즘
- 신경망(딥러닝)의 기원이 되는 알고리즘
- 다수의 신호를 입력으로 받아 하나의 신호를 출력
- 그림의 원은 **뉴런** 또는 노드라고 함
- 각 입력 신호가 뉴련에 보내 질때는 각각 고유한 **가중치** 가 곱해진다. 그 신호의 총합이 정해진 한계 를 넘어설 때만 1을 출력한다. 그 한계를 **임계값** 이라한다.
- 논리회로 게이트들을 퍼셉트론을 활용하여 구현한다. 핵심은 각 게이트의 진리표 즉, 두 입력값과 결과값을 만족하는 **매개변수(가중치와 임계값)** 를 직접 정하고 퍼셉트론의 수식을 적용하여 구현한다.
- 기존의 퍼셉트론 수식에서 임계값을 -b로 치환하여 수식을 바꾼다.
- `w1\*x1 + w2\*x2 <= 임계값 이면 0, w1\*x1 + w2\*x2 > 임계값 이면 1` 이 식에서 임계값을 -b로 치환하여 `b + w1\*x1 + w2\*x2 <= 0 이면 0, b + w1\*x1 + w2\*x2 > 0 이면 1` 로 변경
- 바뀐 식에서 b를 **편향(bias)** 라고 한다.
- _가중치는 입력 신호가 결과에 주는 영향력(중요도)를 조절하는 매개변수고, 편향은 뉴런이 얼마나 쉽게 활성화(결과로 1을 출력)하느냐를 조정하는 매개변수._
- 그런데 XOR게이트(배타적 논리합)는 지금까지의 퍼셉트론으로는 구현할 수 없다. XOR 진료표를 만족하는 매개변수를 찾고 (-0.5, 1.0, 1.0), 두 입력신호에 대한 출력값을 그래프로 그리면 나머지 것들은 직선(선형)으로 나눌 수 있다. (1을 출력하는 영역과 0을 출력하는 영역으로) 그런데 XOR은 직선으로 되지 않고 곡선(비선형))으로 나눌 수 있다.
- 대신 퍼셉트론을 '층을 쌓아' 다층 퍼셉트론(multi-layer-perceptron)으로 구현할 수 있다. XOR 게이트를 다른 나머지 게이트(AND, OR, NAND)의 조합으로 구현. **다층구조의 네트워크**
- **단층 퍼셉트론으로는 표현하지 못한 것을 층을 하나 늘려 구현**

# 신경망
- 입력층(0층), 은닉층(1층), 출력층(2층)으로 구성. 은닉층은 사람 눈에 보이지 않는다.
- 기존 네트워크에서 편향 b를 추가한다. 항상 입력값이 1인 노드로 대신 가중치가 b가 된다.
- 조건 분기의 동작(0을 넘으면 1을 출력하고 그렇지 않으면 0을 출력)을 하나의 함수로 h(x) 표현.
- 출력 y = h(b + w1\*x1 + w2\*x2),  h(x) : x <= 0 이면 0, x > 0 면 1
- 입력 신호의 총합을 출력 신호로 변환하는 함수를 **활성화 함수 activation function** 라 한다.
- a = b + w1\*x1 =+ w2\*x2 , y = h(a)
- 활성화 함수의 처리과정을 명시적으로 나타내어서 2단계로 구분함.
- 일반적으로 **단순 퍼셉트론** 은 단층 네트워크에서 계단함수(임계값을 경계로 출력이 바뀌는 함수)를 활성함수로 사용한 모델을 가리키고 **다층 퍼셉트론** 은 신경망(여러 층으로 구성되고 시그모이드 함수 등의 매끈한 활성화 함수를 사용하는 네트워크)를 가리킨다.

### 활성화 함수
- 앞의 퍼셉트론에서 사용한 0보다 크면 1, 아니면 0 과 같은 임계값을 기준으로 출력이 바뀌는 함수를 **계단 함수 step function** 라 한다.
- 신경망에서 자주 이용하는 활성화 함수 **시그모이드 함수 sigmoid function**
- h(x) = 1 / 1 + exp(-x)
- exp(-x)는 e의 -x승을 뜻하며, e는 자연상수로 2.7182...의 값을 갖는 실수. exp 는 numpy.exp() 함수를 이용하면 된다.
- `여기서부터 살짝 맨탈이 무너진다. 자연상수라니... 일단 그런가보다 하고 넘어가자`
- 시그모이드 함수도 입력을 주면 출력을 돌려주는 변환기. h(1.0) = 0.731... , h(2.0) = 0.880... 처럼 특정 값을 출력한다. (0 과 1이 아닌)
- 계단 함수의 그래프는 임계값(여기서 0)을 기준으로 계단처럼 생겼다.
- 시그모이드 함수의 그래프는 S자 모양을 나타낸다.
- 두 함수의 차이는 '매끄러움'의 차이. 시그모이드 함수는 부드러운 곡선이며 입력에 따라 출력이 연속적으로 변화한다. (계단 함수는 0을 기준으로 갑자기 바뀐다.)
- 공통점은 입력이 작을 때의 출력은 0에 가깝고 (혹은 0), 입력이 커지면 출력이 1에 가까워지는 구조. 입력이 아무리 작거나 커도 출력은 0에서 1사이라는 것.
- 그리고 둘다 비선형 함수다.
- 변환기(함수)에 무언가 입력했을 때 출력이 입력의 상수배만큼 변하는 함수를 **선형 함수** 라고 합니다. 수식으로는 f(x) = a\*x + b 이고 이떄 a와 b는 상수. 그래서 선형 함수는 곧은 1개의 직선이 된다. 한편 **비선형 함수** 는 문자 그대로 '선형이 아닌' 함수. 즉, 직선 1개로는 그릴 수 없는 함수

```
신경망에서는 활성화 함수로 비선형 함수를 사용해야 합니다. 달리 말하면 선형 함수를 사용해서는 안 됩니다. 선형 함수를 이용하면 신경망의 층을 깊게 하는 의미가 없어지기 때문.

선형 함수의 문제는 층을 아무리 깊게 해도 '은닉층이 없는 네트워크'로도 똑같은 기능을 할 수 있다는 데 있습니다. (즉, 층을 깊게해도 의미가 없다는 위 문단의 뜻.)

선형 함수인 h(x) = cx 를 활성화 함수로 사용한 3층 네트워크를 떠올려보세요. 이를 식으로 나타내면 y(x) = h(h(h(x))) 가 됩니다. (아.. 대충 이해하면 3층 네트워크면 가중치를 갖는 층이 3개라는 의미이니 각 가중치가 계산된 층의 활성화 함수가 3개라고 이해하면 될까. 그래서 각 활성화 함수에서는 바로 전 층의 활성화 함수를 x로 받는 다고 보면 될까.) 이 계산은 y(x) = c * c * c * x 처럼 세번의 곱셈을 수행하지만, 실은 y(x) = ax와 똑같은 식이다. a = c의 3승 이라고 하면 끝. 즉 은닉층이 없는 네트워크로 표현할 수 있다. 결론은 층을 쌓는 혜택을 얻고 싶다면 활성화 함수로는 반드시 비선형 함수를 사용해야 한다!
```
- 최근에는 ReLU (Rectified Linear Unit, 렐루) 함수를 주로 신경망 분야에서 이용. 입력이 0을 넘으면 그 입력을 그대로 출력하고, 0 이하면 0을 출력하는 함수.

```
def relu(x):
        return np.maximum(0, x)
```

- 넘파이의 maximum 함수는 두 입력 중 큰 값을 선택해 반환하는 함수임.

### 다차원 배열의 계산
- 신경망 네트워크에서 각 입력과 가중치 등의 계산에 다차원 배열의 계산을 이용해야 하기에 이에 대한 계산을 학습.
- np.ndim() 은 배열의 차원을 , A.shape 은 배열의 형상을,
- 2차원 배열은 특히 **행렬**metrix 라고 부르고, 배열의 가로 방향을 **행**row, 세로 방향을 **열**column 이라고 한다.
- 행렬의 곱에서는 피연산자의 순서가 다르면 결과도 다르다.
- 행렬 A의 1번째 차원의 원소 수(열 수)와 행렬 B의 0번째 차원의 원소 수(행 수)가 같아야 한다.
- 여기서 부터는 표기법이 복잡해져서 쓰기가 힘듬.

### 각 층의 신호 전달 구현하기
- 1층의 첫 번째 뉴런의 수식은 가중치를 곱한 신호 두 개와 편향을 합한 식.
- 가중치 부분을 간소화하면 A = XW + B, 요걸 행렬로 나타내면

```
X = np.array([1.0, 0.5])
W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
B1 = np.array([0.1, 0.2, 0.3])

print(W1.shape) # (2, 3)
print(X.shape) # (2, )
print(B1.shape) # (3, )

A1 = np.dot(X, W1) + B1
```

- 각층의 신호 전달을 위와 같이 반복해서 구현 할 수 있다. 마지막 출력층에서는 활성화 함수만 그 앞의 은닉층과 다르다.
- 출력층의 활성화 함수는 풀고자 하는 문제의 성질에 맞게 정한다. 예로 회귀에는 항등 함수를, 2클래스 분류에는 시그모이드 함수를, 다중 클래스 분류에는 소프트맥스 함수를 사용하는 것이 일반적.
- 신호가 순방향으로 전달(입력에서 출력방향)되는 것을 순전파. 그 반대의 역방향(출력에서 입력 방향) 처리도 있다.
- 신경망은 분류와 회귀 모두 이용할 수 있고, 어떤 문제냐에 따라 출력층에서 사용하는 활성화 함수가 달라짐. 회귀에는 항등 함수, 분류에는 소프트맥스 함수가 일반적.

```
기계학습 문제는 분류classification와 회귀regression로 나뉩니다. 분류는 데이터가 어느 클래스에 속하느냐는 문제. 회귀문제는 입력 데이터에서 (연속적인) 수치를 예측하는 문제
```

```
분류와 달리 회귀라는 이름은 직관적이지 않죠. 그 이유를 알려면 이름의 기원을 찾아봐야 합니다.19세기 후반 영국의 우생학자 프랜시스 골턴 경은 사람과 완두콩 등을 대상으로 그 키(크기)를 측정했습니다. 관찰 결과 키가 큰 부모의 자식은 부모보다 작고 작은 부모의 자식은 부모보다 큰, 즉 평균으로 회귀(regression)하는 경향이 있음을 알았습니다. 그 사이에는 선형 관계가 있어 부모의 키로부터 자식의 키를 예측할 수 있고, 그 예측 결괏값이 연속적인 수치인 것이죠. p90 옮긴이주석
```

#### 항등 함수 identify function
- 입력을 그대로 출력한다. 입력과 출력이 항상 같다는 뜻.

#### 소프트맥스 함수 softmax function

- ![https://farm5.staticflickr.com/4112/34857307623_d955255f05_o.gif](https://farm5.staticflickr.com/4112/34857307623_d955255f05_o.gif)
- exp(x)는 edml x승을 뜻하는 지수 함수exponential function. e는 자연상수. n은 출력층의 뉴런 수. 분모는 모든 입력 신호의 지수 함수의 합.
- 핵심은 출력층의 각 뉴런이 모든 입력 신호에서 영향을 받는다!
- 지수함수를 사용하다보니 수가 아주 커집. 큰 값끼리 나눗셈을 하면 결과 수치가 '불안정'
- 지수 함수를 계산할 떄 어떤 정수를 더해도 (혹은 빼도)결과는 바뀌지 않는다.
- ![https://farm5.staticflickr.com/4259/35501868572_1eb5190536_o.gif](https://farm5.staticflickr.com/4259/35501868572_1eb5190536_o.gif)
- **소프트맥스 함수의 출력은 0에서 1.0 사이의 실수. 그리고 출력의 총합은 1. 이 성질 덕분에 소프트맥스 함수의 출력을 '확률'로 해석할 수 있다.**
- 소프트맥스 함수를 적용해도 각 원소의 대소 관계는 변하지 않는다. 즉, 지수 함수 y = exp(x)가 단조 증가 함수기 때문. a의 원소들 사이의 대소 관계가 y의 원소들 사이의 대소 관계로 그대로 이어진다. 
- 신경망의 분류에서는 일반적으로 가장 큰 출력을 내는 뉴런에 해당하는 클래스만 인식한다. 그리고 소프트맥스 함수를 적용해도 출력이 가장 큰 뉴런의 위치는 달라지지 않는다. 결과적으로 신경망으로 분류할 때는 출력층의 소프트맥스 함수를 생략해도 된다. 현업에서도 지수 함수 계산에 드는 자원 낭비를 줄이고자 출력층의 소프트맥스 함수는 생각하는 것이 일반적.

```
기계학습의 문제 풀이는 학습과 추론inference의 두 단계를 거쳐 이뤄집니다. 학습 단계에서 모델을 학습하고(직업 훈련을 받고), 추론 단계에서 앞서 학습한 모델로 미지의 데이터에 대해서 추론(분류)을 수행합니다.(현장에 나가 진짜 일을 합니다). 방금 설명한 대로 추론 단계에서는 출력층의 소프트맥스 함수를 생략하는 것이 일반적입니다. 한편, 신경망을 학습시킬 떄는 출력층에서 소프트맥스 함수를 사용합니다.
```

#### 출력층의 뉴런 수 정하기

- 출력층의 뉴런 수는 풀려는 문제에 맞게 적절히 정해야한다.
- 분류에서는 분류하고 싶은 클래스 수로 설정하는 것이 일반적!

### 손글씨 숫자 인식
- 이미 학습되니 매개변수를 사용하여 학습 과정은 생략하고, 추론 과정만 구현.
- 추론 과정을 신경망의 순전파forward propagation 라고도 한다.
- 데이터를 특정 범위로 변환하는 처리를 정규화normalization라고 하고, 신경망의 입력 데이터에 특정 변환을 가하느ㄴ 것을 전처리pre-processing이라고 한다.

# 신경망 학습
- 퍼셉트론도 직선으로 분리할 수 있는(선형 분리 가능) 문제라면 데이터로부터 자동으로 학습할 수 있다. 퍼셉트론 수렴 정리perceptron convergence theorem로 정리되어 있다. 하지만 비선형 분리 문제는 (자동으로) 학습할 수 없다.
- 사람은 경험과 직관을 단서로 시행착오를 거듭하며 일을 진행하며 문제를 해결.
- 기계학습에서는 사람의 개입을 최소화하고 수집한 데이토로부터 패턴을 찾으려 시도. 신경망과 딥러닝은 기존 기계학습에서 사용하던 방법보다 사람의 개입을 더욱 배제할 수 있게 해주는 중요한 특성을 지님.
- 손글씨 이미지 인식의 예로 이미지에서 **특징**feature을 추출하고 그 특징의 패턴을 기계학습 기술로 학습하는 방법. 여기서 특징은 입력 데이터(입력 이미지)에서 본질적인 데이터(중요한 데이터)를 정확하게 추출할 수 있도록 설계된 변환기를 가리친다.
- 본질적인 데이터 즉 중요한 데이터라는건 뭐지? 이런 본질적인 데이터, 즉 특징은 각 분야마다 이미 만들어 진 것들이 있다? 이미지 특징은 보통 벡터로 기술하고, 컴퓨터 비전 분야에서는 SIFT, SURF, HOG등의 특징을 많이 사용한다고... 요런 것들을 사용하면 이미지 데이터를 벡터로 변환하고, 변환된 벡터를 가지고 지도 학습 방식의 대표 분류 기법인 SVM, KNN 등으로 학습할 수 있다.
- 다만, 이미지를 벡터로 변환할 때 사용하는 특징은 여전히 '사람'이 설계하는 것. 위에서 말한 SIFT, SURF, HOG 등의 특징을 말하는 것인듯.
- 그런데, 신경망(딥러닝) 방식은 그런 특징마저 기계가 스스로 찾는다!! 이미지에 포함된 중요한 특징까지도 '기계'가 스스로 학습.

```
딥러닝을 **종단간 기계학습**end-to-end machine learning이라고도 한다. 여기서 종단간은 '처음부터 끝까지'라는 의미로, 데이터(입력)에서 목표한 결과(출력)을 얻는다는 뜻.
```

- 기계학습 문제는 데이터를 **훈련 데이터**training data와 **시험 데이터**test data로 나눠 학습과 실험을 수행. 범융적 사용을 위해.
- 한 데이터셋에만 지나치게 최적화된 상태를 **오버피팅**overfitting이라고 한다.

### 손실 함수
- 신경망 학습에서는 현재의 상태를 '하나의 지표'로 표현한다. ++그 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것!!++
- 신경망 학습에서 사용하는 지표는 **손실 함수**loss function. 일반적으로 평균 제곱 오차와 교차 엔트로피 오차를 사용한다.

```
손실 함수는 신경망 성능의 '나쁨'을 나타내는 지표로, 현재의 신경망이 훈련 데이터를 얼마나 잘 처리하지 '못'하느냐를 나타냄. 손실 함수에 마이너스만 곱하면 '얼마나 좋으냐'라는 지표로도 변신
```

#### 평균 제곱 오차

#### 교차 엔트로피 오차

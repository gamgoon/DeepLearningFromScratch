# Perceptron 퍼셉트론 알고리즘
- 신경망(딥러닝)의 기원이 되는 알고리즘
- 다수의 신호를 입력으로 받아 하나의 신호를 출력
- 그림의 원은 **뉴런** 또는 노드라고 함
- 각 입력 신호가 뉴련에 보내 질때는 각각 고유한 **가중치** 가 곱해진다. 그 신호의 총합이 정해진 한계 를 넘어설 때만 1을 출력한다. 그 한계를 **임계값** 이라한다.
- 논리회로 게이트들을 퍼셉트론을 활용하여 구현한다. 핵심은 각 게이트의 진리표 즉, 두 입력값과 결과값을 만족하는 **매개변수(가중치와 임계값)** 를 직접 정하고 퍼셉트론의 수식을 적용하여 구현한다.
- 기존의 퍼셉트론 수식에서 임계값을 -b로 치환하여 수식을 바꾼다.
- `w1\*x1 + w2\*x2 <= 임계값 이면 0, w1\*x1 + w2\*x2 > 임계값 이면 1` 이 식에서 임계값을 -b로 치환하여 `b + w1\*x1 + w2\*x2 <= 0 이면 0, b + w1\*x1 + w2\*x2 > 0 이면 1` 로 변경
- 바뀐 식에서 b를 **편향(bias)** 라고 한다.
- _가중치는 입력 신호가 결과에 주는 영향력(중요도)를 조절하는 매개변수고, 편향은 뉴런이 얼마나 쉽게 활성화(결과로 1을 출력)하느냐를 조정하는 매개변수._
- 그런데 XOR게이트(배타적 논리합)는 지금까지의 퍼셉트론으로는 구현할 수 없다. XOR 진료표를 만족하는 매개변수를 찾고 (-0.5, 1.0, 1.0), 두 입력신호에 대한 출력값을 그래프로 그리면 나머지 것들은 직선(선형)으로 나눌 수 있다. (1을 출력하는 영역과 0을 출력하는 영역으로) 그런데 XOR은 직선으로 되지 않고 곡선(비선형))으로 나눌 수 있다.
- 대신 퍼셉트론을 '층을 쌓아' 다층 퍼셉트론(multi-layer-perceptron)으로 구현할 수 있다. XOR 게이트를 다른 나머지 게이트(AND, OR, NAND)의 조합으로 구현. **다층구조의 네트워크**
- **단층 퍼셉트론으로는 표현하지 못한 것을 층을 하나 늘려 구현**

# 신경망
- 입력층(0층), 은닉층(1층), 출력층(2층)으로 구성. 은닉층은 사람 눈에 보이지 않는다.
- 기존 네트워크에서 편향 b를 추가한다. 항상 입력값이 1인 노드로 대신 가중치가 b가 된다.
- 조건 분기의 동작(0을 넘으면 1을 출력하고 그렇지 않으면 0을 출력)을 하나의 함수로 h(x) 표현.
- 출력 y = h(b + w1\*x1 + w2\*x2),  h(x) : x <= 0 이면 0, x > 0 면 1
- 입력 신호의 총합을 출력 신호로 변환하는 함수를 **활성화 함수 activation function** 라 한다.
- a = b + w1\*x1 =+ w2\*x2 , y = h(a)
- 활성화 함수의 처리과정을 명시적으로 나타내어서 2단계로 구분함.
- 일반적으로 **단순 퍼셉트론** 은 단층 네트워크에서 계단함수(임계값을 경계로 출력이 바뀌는 함수)를 활성함수로 사용한 모델을 가리키고 **다층 퍼셉트론** 은 신경망(여러 층으로 구성되고 시그모이드 함수 등의 매끈한 활성화 함수를 사용하는 네트워크)를 가리킨다.

## 활성화 함수
- 앞의 퍼셉트론에서 사용한 0보다 크면 1, 아니면 0 과 같은 임계값을 기준으로 출력이 바뀌는 함수를 **계단 함수 step function** 라 한다.
- 신경망에서 자주 이용하는 활성화 함수 **시그모이드 함수 sigmoid function**
- h(x) = 1 / 1 + exp(-x)
- exp(-x)는 e의 -x승을 뜻하며, e는 자연상수로 2.7182...의 값을 갖는 실수. exp 는 numpy.exp() 함수를 이용하면 된다.
- `여기서부터 살짝 맨탈이 무너진다. 자연상수라니... 일단 그런가보다 하고 넘어가자`
- 시그모이드 함수도 입력을 주면 출력을 돌려주는 변환기. h(1.0) = 0.731... , h(2.0) = 0.880... 처럼 특정 값을 출력한다. (0 과 1이 아닌)
- 계단 함수의 그래프는 임계값(여기서 0)을 기준으로 계단처럼 생겼다.
- 시그모이드 함수의 그래프는 S자 모양을 나타낸다.
- 두 함수의 차이는 '매끄러움'의 차이. 시그모이드 함수는 부드러운 곡선이며 입력에 따라 출력이 연속적으로 변화한다. (계단 함수는 0을 기준으로 갑자기 바뀐다.)
- 공통점은 입력이 작을 때의 출력은 0에 가깝고 (혹은 0), 입력이 커지면 출력이 1에 가까워지는 구조. 입력이 아무리 작거나 커도 출력은 0에서 1사이라는 것.
- 그리고 둘다 비선형 함수다.
- 변환기(함수)에 무언가 입력했을 때 출력이 입력의 상수배만큼 변하는 함수를 **선형 함수** 라고 합니다. 수식으로는 f(x) = a\*x + b 이고 이떄 a와 b는 상수. 그래서 선형 함수는 곧은 1개의 직선이 된다. 한편 **비선형 함수** 는 문자 그대로 '선형이 아닌' 함수. 즉, 직선 1개로는 그릴 수 없는 함수

# 3장 신경망

## 시그모이드 함수와 계단 함수

- 둘 다 입력이 작을 떄의 출력은 0에 가깝고, 입력이 커지면 출력이 1에 가까워진다. 즉, 계단 함수와 시그모이드 함수는 입력이 중요하면 큰 값을 출력하고 입력이 중요하지 않으면 작은 값을 출력한다.
- 입력이 아무리 작거나 커도 출력은 0에서 1 사이다.
- 둘 다 비선형 함수다.


> 출력이 입력의 상수배만큼 변하는 함수를 선형 함수. 수식으로는 f(x) = ax + b 이고, 이때 a 와 b 는 상수. 선형함수는 곧은 1개의 직선이 된다. 한편 비선형 함수는 '선형이 아닌' 함수. 즉, 직선 1개로 그릴 수 없는 함수.


### 시그모이드

- 입력에 따라 출력이 연속적으로 변화
- 실수를 돌려준다. 즉 퍼셉트론에서는 뉴런 사이에 0 혹은 1이 흐른다.


### 계단 함수

- 0을 경계로 출력이 갑자기 바뀐다.
- 0과 1 중 하나의 값만 돌려준다. 즉 신경망에서는 연속적인 실수가 흐른다.
